# Load necessary libraries
library(readr)    # For reading CSV files
library(caTools)  # For data splitting
library(xgboost)  # For the model

# Load the dataset
crime_data <- read_csv("crime_dataset_india.csv")

# Drop unnecessary columns
crime_data_model <- subset(crime_data, select = -c(`Report Number`, `Date Reported`, 
                                                   `Date of Occurrence`, `Time of Occurrence`, 
                                                   `Date Case Closed`, `Case Closed`))

# Encode target variable (1 for Violent Crime, 0 for Non-Violent)
crime_data_model$`Crime Domain` <- ifelse(crime_data_model$`Crime Domain` == "Violent Crime", 1, 0)

# Simplified Data Preprocessing
crime_data_model$City <- as.factor(crime_data_model$City)
crime_data_model$`Crime Description`<- as.factor(crime_data_model$`Crime Description`)
crime_data_model$`Victim Gender` <- as.factor(crime_data_model$`Victim Gender`)
crime_data_model$`Weapon Used` <- as.factor(crime_data_model$`Weapon Used`)

# Convert factors to numeric (carefully manage levels)
crime_data_model$City <- as.numeric(crime_data_model$City)
crime_data_model$`Crime Description` <- as.numeric(crime_data_model$`Crime Description`)
crime_data_model$`Victim Gender` <- as.numeric(crime_data_model$`Victim Gender`)
crime_data_model$`Weapon Used` <- as.numeric(crime_data_model$`Weapon Used`)
crime_data_model$`Police Deployed` <- as.numeric(crime_data_model$`Police Deployed`)

# Data Splitting (reproducible)
set.seed(123)
# Get total row count
total_rows <- nrow(crime_data_model)

# Create a shuffled sequence of row indices
shuffled_indices <- sample(total_rows)

# Define split point (80% training, 20% testing)
split_point <- round(0.8 * total_rows)

# Assign indices for training and testing sets
train_indices <- shuffled_indices[1:split_point]
test_indices <- shuffled_indices[(split_point + 1):total_rows]

# Create datasets using indices (no overlap possible)
train_data <- crime_data_model[train_indices, , drop = FALSE]
test_data <- crime_data_model[test_indices, , drop = FALSE]

# Verify no overlap
overlap <- intersect(train_indices, test_indices)

if (length(overlap) == 0) {
  print("✅ No overlapping rows found. Data split is correct!")
} else {
  print(paste("❌ Overlap detected! Number of overlapping rows:", length(overlap)))
}

train_matrix <- as.matrix(subset(train_data, select = -`Crime Domain`))
test_matrix <- as.matrix(subset(test_data, select = -`Crime Domain`))

# Define Target Variable (Labels)
train_labels <- train_data$`Crime Domain`
test_labels <- test_data$`Crime Domain`


# Define XGBoost Parameters (tuned)
params <- list(
  objective = "binary:logistic",
  eval_metric = "error",
  eta = 0.01,  # Reduce learning rate
  max_depth = 6,  # Reduce max_depth
  subsample = 0.8,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  gamma = 0.1,  # Add gamma for regularization
  lambda = 0.7,    # L2 regularization
  alpha = 0.1      # L1 regularization
)

# Data Formatting
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest <- xgb.DMatrix(data = test_matrix, label = test_labels)

# Train XGBoost Model
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 500, # Increased nrounds
  watchlist = list(train = dtrain, eval = dtest),
  early_stopping_rounds = 50, # Adjusted early stopping
  verbose = 1
)

# Evaluate Model
predictions <- predict(xgb_model, test_matrix)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Confusion Matrix (essential)
conf_matrix <- confusionMatrix(data = factor(predicted_classes, levels = c(0, 1)),
                               reference = factor(test_labels, levels = c(0, 1)))
print(conf_matrix)

# Feature Importance (validate inputs)
importance_matrix <- xgb.importance(model = xgb_model)
print(importance_matrix)


accuracy <- sum(predicted_classes == test_labels) / length(test_labels)
print(paste("✅ Model Accuracy:", round(accuracy * 100, 2), "%"))

# Load necessary libraries
library(xgboost)

# Make sure 'Police.Deployed' column is numeric
train_data$`Police Deployed` <- as.numeric(train_data$`Police Deployed`)

# Ensure the new input tuple matches the preprocessed format
# Replace values below with your actual input values
new_tuple <- data.frame(
  City = as.numeric(factor("Delhi", levels = levels(as.factor(train_data$City)))),
  `Crime Code` = 197, # Numeric value (e.g., Crime Code for ASSAULT)
  `Crime Description` = as.numeric(factor("ASSAULT", levels = levels(as.factor(train_data$`Crime Description`)))),
   `Victim Age` = 61, # Numeric value for victim's age
  `Victim Gender` = as.numeric(factor("F", levels = levels(as.factor(train_data$`Victim Gender`)))),
  `Weapon Used` = as.numeric(factor("KNIFE", levels = levels(as.factor(train_data$`Weapon Used`)))),
  `Police Deployed` = 10 # Numeric value representing the number of police deployed
)

# Ensure column names match exactly with training data feature names
colnames(new_tuple) <- colnames(train_matrix)

# Convert the new tuple to matrix format (required by xgboost)
new_tuple_matrix <- as.matrix(new_tuple)

# Predict using the trained XGBoost model
new_prediction <- predict(xgb_model, new_tuple_matrix)

# Convert probability to class label (0 for nonviolent, 1 for violent)
predicted_class <- ifelse(new_prediction > 0.5, "Violent Crime", "Nonviolent Crime")

# Output the result
print(paste("Predicted Crime Domain:", predicted_class))


table(crime_data_model$`Crime Domain`)
print(new_prediction)

